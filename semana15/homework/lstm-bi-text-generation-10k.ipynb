{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dpalominop/SistemasInteligentes/blob/master/semana15/homework/lstm-bi-text-generation-10k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_qFRwvLVL9Qs"
   },
   "source": [
    "# Test Generation using a Bidirectional LSTM Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B1Eqhf6JL9Qy"
   },
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "colab_type": "code",
    "id": "dWcJZBNJL9Q1",
    "outputId": "29673dcb-ff18-44ac-ad83-93a6bafb4f87"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#import Keras library\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Input, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LambdaCallback\n",
    "from keras.metrics import categorical_accuracy\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.device('/gpu:1')\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "#config = tf.ConfigProto(intra_op_parallelism_threads=4, \\\n",
    "#                        inter_op_parallelism_threads=4, \\\n",
    "#                        allow_soft_placement=True,\\\n",
    "#                        device_count = {'CPU' : 1, 'GPU' : 2})\n",
    "\n",
    "#sess = tf.Session(config=config)\n",
    "#K.set_session(sess)\n",
    "\n",
    "#import spacy, and spacy french model\n",
    "# spacy is used to work on text\n",
    "#!pip install spacy\n",
    "import spacy\n",
    "#!python -m spacy download es\n",
    "nlp = spacy.load('es')\n",
    "\n",
    "#import other libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "bZ4qMpiRMSOs",
    "outputId": "dba41921-fe69-4724-a2f0-699d9d385940"
   },
   "outputs": [],
   "source": [
    "gdrive = False\n",
    "if gdrive:\n",
    "    !pip install -U -q PyDrive\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    "\n",
    "    # Authenticate and create the PyDrive client.\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)\n",
    "    \n",
    "    !curl https://raw.githubusercontent.com/dexterfichuk/GoogleDriveCheckpoint/master/google_drive_checkpoint.py -O\n",
    "      \n",
    "    from google_drive_checkpoint import GoogleDriveCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v77u-lhBL9RF"
   },
   "outputs": [],
   "source": [
    "#define parameters used in the tutorial\n",
    "data_dir = 'data/'# data directory containing raw texts\n",
    "save_dir = 'models/' # directory to store trained NN models\n",
    "file_list = [\"spanish_emojis\"]\n",
    "\n",
    "if gdrive:\n",
    "    link = \"https://drive.google.com/open?id=1kdV9lTy7RNp4xYX6_IMA-6wfMRyXNSgv\"\n",
    "    fluff, id = link.split('=')\n",
    "    downloaded = drive.CreateFile({'id':id}) \n",
    "    downloaded.GetContentFile('words_vocab.pkl')\n",
    "    vocab_file = \"words_vocab.pkl\" \n",
    "    \n",
    "    link = \"https://drive.google.com/open?id=1xr0ZW6lBDTALEqaAdZefu-DwgEPIgM83\"\n",
    "    fluff, id = link.split('=')\n",
    "    downloaded = drive.CreateFile({'id':id}) \n",
    "    downloaded.GetContentFile('words_list.pkl')\n",
    "    words_list_file = \"words_list.pkl\" \n",
    "    \n",
    "    link = \"https://drive.google.com/open?id=17o5LV6AizHqPw9QeIjKwFXmTASpNs673\"\n",
    "    fluff, id = link.split('=')\n",
    "    downloaded = drive.CreateFile({'id':id}) \n",
    "    downloaded.GetContentFile('spanish_emojis.csv')\n",
    "    data_file = \"spanish_emojis.csv\"\n",
    "    \n",
    "    link = \"https://drive.google.com/open?id=1gTUqnhuknwFP4BIFQixRIjB7V-CQyb_r\"\n",
    "    fluff, id = link.split('=')\n",
    "    downloaded = drive.CreateFile({'id':id}) \n",
    "    downloaded.GetContentFile('sequences_list.pkl')\n",
    "    sequences_file = \"sequences_list.pkl\" \n",
    "    \n",
    "    link = \"https://drive.google.com/open?id=13NEQdiBBn-3vCy8Q5JF-OBzrD7hs680j\"\n",
    "    fluff, id = link.split('=')\n",
    "    downloaded = drive.CreateFile({'id':id}) \n",
    "    downloaded.GetContentFile('next_words_list.pkl')\n",
    "    next_words_file = \"next_words_list.pkl\"\n",
    "    \n",
    "else:   \n",
    "    vocab_file = os.path.join(data_dir, \"words_vocab.pkl\")\n",
    "    words_list_file = os.path.join(data_dir, \"words_list.pkl\")\n",
    "    data_file = os.path.join(data_dir, file_list[0] + \".csv\")\n",
    "    sequences_file = os.path.join(data_dir, \"sequences_list.pkl\")\n",
    "    next_words_file = os.path.join(data_dir, \"next_words_list.pkl\")\n",
    "\n",
    "sequences_step = 1 #step to create sequences\n",
    "seq_length = 10 # sequence length\n",
    "preprocess = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "herJ9AX3L9RO"
   },
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    items = []\n",
    "    if os.path.exists(filename):\n",
    "        # try:\n",
    "        with open(filename, 'rb') as fname:\n",
    "            while True:\n",
    "                try:\n",
    "                    items = cPickle.load(fname)\n",
    "                except EOFError:\n",
    "                    print(EOFError)\n",
    "                    break\n",
    "    else:\n",
    "        items = []\n",
    "    \n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CaMf9GfIL9RU"
   },
   "outputs": [],
   "source": [
    "def create_wordlist(doc):\n",
    "    wl = []\n",
    "    for word in doc:\n",
    "        if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "            wl.append(word.text.lower())\n",
    "    return wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lmDYsfBLL9RZ"
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "next_words = []\n",
    "wordlist = []\n",
    "if(preprocess):\n",
    "    input_file = pd.read_csv(data_file)\n",
    "    lines = input_file['observations']\n",
    "    for data in lines:\n",
    "        #create sentences\n",
    "        doc = nlp(data)\n",
    "        wl = create_wordlist(doc)\n",
    "        wordlist = wordlist + wl\n",
    "        if(len(wl) < 2): \n",
    "            continue\n",
    "        seq = [' ' for i in range(seq_length - len(wl) + 1)]\n",
    "        seq = seq + wl\n",
    "\n",
    "        for i in range(0, len(seq) - seq_length, sequences_step):\n",
    "          sequences.append(seq[i: i + seq_length])\n",
    "          next_words.append(seq[i + seq_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYPBqN0JL9Rf"
   },
   "outputs": [],
   "source": [
    "if(preprocess):\n",
    "    # count the number of words\n",
    "    word_counts = collections.Counter(wordlist)\n",
    "\n",
    "    # Mapping from index to word : that's the vocabulary\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "\n",
    "    # Mapping from word to index\n",
    "    vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    words = [x[0] for x in word_counts.most_common()]\n",
    "\n",
    "    #size of the vocabulary\n",
    "    vocab_size = len(words)\n",
    "    print(\"vocab size: \", vocab_size)\n",
    "\n",
    "    #save the words and vocabulary\n",
    "    with open(os.path.join(vocab_file), 'wb') as f:\n",
    "        cPickle.dump((words, vocab, vocabulary_inv), f)\n",
    "    with open(os.path.join(words_list_file), 'wb') as f:\n",
    "        cPickle.dump((wordlist), f)\n",
    "    with open(os.path.join(sequences_file), 'wb') as f:\n",
    "        cPickle.dump((sequences), f)\n",
    "    with open(os.path.join(next_words_file), 'wb') as f:\n",
    "        cPickle.dump((next_words), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "l--bcrWRL9Rl",
    "outputId": "85ff0a09-5846-417a-b519-dc8d568c8b51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'EOFError'>\n",
      "<class 'EOFError'>\n",
      "<class 'EOFError'>\n",
      "<class 'EOFError'>\n",
      "vocab size:  66846\n"
     ]
    }
   ],
   "source": [
    "if(not preprocess):\n",
    "    wordlist = read_file(words_list_file)\n",
    "    (words, vocab, vocabulary_inv) = read_file(vocab_file)\n",
    "    sequences = read_file(sequences_file)\n",
    "    next_words = read_file(next_words_file)\n",
    "    \n",
    "    # count the number of words\n",
    "    word_counts = collections.Counter(wordlist)\n",
    "    \n",
    "    #size of the vocabulary\n",
    "    vocab_size = len(words)\n",
    "    print(\"vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "4rN3yd8QL9Rv",
    "outputId": "f44a9ed7-ed88-4c33-ae72-b2c8903bfcca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences:  421621\n",
      "vocav( ):  0\n",
      "words(0):   \n"
     ]
    }
   ],
   "source": [
    "#create sequences\n",
    "#sequences = []\n",
    "#next_words = []\n",
    "#for i in range(0, len(wordlist) - seq_length, sequences_step):\n",
    "#    sequences.append(wordlist[i: i + seq_length])\n",
    "#    next_words.append(wordlist[i + seq_length])\n",
    "\n",
    "print('nb sequences: ', len(sequences))\n",
    "print('vocav( ): ', vocab[' '])\n",
    "print('words(0): ', vocabulary_inv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-iEN3bNgL9R_"
   },
   "outputs": [],
   "source": [
    "def bidirectional_lstm_model(seq_length, vocab_size):\n",
    "    print('Build LSTM model.')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5akgUQYLL9SI"
   },
   "outputs": [],
   "source": [
    "rnn_size = 40 # size of RNN\n",
    "batch_size = 32 # minibatch size\n",
    "seq_length = 10 # sequence length\n",
    "num_epochs = 20 # number of epochs\n",
    "learning_rate = 0.01 #learning rate\n",
    "sequences_step = 1 #step to create sequences\n",
    "BATCH_GEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "cYYiskZQL9SM",
    "outputId": "7fac6c4d-9b19-4bfa-9cc7-6f46d0a6a563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 80)                21403840  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 66846)             5414526   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 66846)             0         \n",
      "=================================================================\n",
      "Total params: 26,818,366\n",
      "Trainable params: 26,818,366\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "md = bidirectional_lstm_model(seq_length, vocab_size)\n",
    "md.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6RDgEznwzVE"
   },
   "outputs": [],
   "source": [
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, seq_length, len(words)), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index]):\n",
    "                x[i, t, vocab[w]] = 1\n",
    "            y[i, vocab[next_words[index]]] = 1\n",
    "\n",
    "            index = index + 1\n",
    "            if index == len(sentence_list):\n",
    "                index = 0\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LBvSvUgjwzVI"
   },
   "outputs": [],
   "source": [
    "file_path = \"BI-LSTM-epoch{epoch:03d}-words%d-sequence%d\" % (\n",
    "    len(words),\n",
    "    seq_length\n",
    ")\n",
    "checkpoint = None\n",
    "if gdrive:\n",
    "    checkpoint = GoogleDriveCheckpoint(file_path, drive, save_best_only=True, save_weights_only=True, verbose=1, mode='auto', monitor='val_acc')\n",
    "else:\n",
    "    checkpoint = ModelCheckpoint(os.path.join(save_dir, file_path), monitor='val_acc', save_best_only=True)\n",
    "\n",
    "#print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "callbacks_list = [checkpoint, \n",
    "                  #print_callback, \n",
    "                  early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "CQxZQxwQwzVN",
    "outputId": "34ef9ee1-e6e7-43bc-b2a1-4b8f4358e181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "101/101 [==============================] - 76s 749ms/step - loss: 8.5630 - categorical_accuracy: 0.0236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\opt\\Anaconda3\\envs\\SI\\lib\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "D:\\opt\\Anaconda3\\envs\\SI\\lib\\site-packages\\keras\\callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "101/101 [==============================] - 70s 692ms/step - loss: 7.0832 - categorical_accuracy: 0.0272\n",
      "Epoch 3/20\n",
      "101/101 [==============================] - 70s 689ms/step - loss: 6.6337 - categorical_accuracy: 0.0397\n",
      "Epoch 4/20\n",
      "101/101 [==============================] - 69s 687ms/step - loss: 6.2011 - categorical_accuracy: 0.0554\n",
      "Epoch 5/20\n",
      "101/101 [==============================] - 69s 687ms/step - loss: 5.7995 - categorical_accuracy: 0.0775\n",
      "Epoch 6/20\n",
      "101/101 [==============================] - 70s 691ms/step - loss: 5.3744 - categorical_accuracy: 0.1032\n",
      "Epoch 7/20\n",
      "101/101 [==============================] - 70s 691ms/step - loss: 4.8181 - categorical_accuracy: 0.1414\n",
      "Epoch 8/20\n",
      "101/101 [==============================] - 70s 692ms/step - loss: 4.2414 - categorical_accuracy: 0.1802\n",
      "Epoch 9/20\n",
      "101/101 [==============================] - 70s 690ms/step - loss: 3.7431 - categorical_accuracy: 0.2412\n",
      "Epoch 10/20\n",
      "101/101 [==============================] - 69s 687ms/step - loss: 3.1664 - categorical_accuracy: 0.3383\n",
      "Epoch 11/20\n",
      "101/101 [==============================] - 69s 687ms/step - loss: 2.6202 - categorical_accuracy: 0.4315\n",
      "Epoch 12/20\n",
      "101/101 [==============================] - 69s 679ms/step - loss: 2.1116 - categorical_accuracy: 0.5395\n",
      "Epoch 13/20\n",
      "101/101 [==============================] - 67s 665ms/step - loss: 1.8020 - categorical_accuracy: 0.5941\n",
      "Epoch 14/20\n",
      "101/101 [==============================] - 68s 675ms/step - loss: 1.5416 - categorical_accuracy: 0.6436\n",
      "Epoch 15/20\n",
      "101/101 [==============================] - 73s 719ms/step - loss: 1.2661 - categorical_accuracy: 0.7060\n",
      "Epoch 16/20\n",
      "101/101 [==============================] - 69s 681ms/step - loss: 1.1114 - categorical_accuracy: 0.7378\n",
      "Epoch 17/20\n",
      "101/101 [==============================] - 68s 677ms/step - loss: 0.9676 - categorical_accuracy: 0.7695\n",
      "Epoch 18/20\n",
      "101/101 [==============================] - 69s 679ms/step - loss: 0.8303 - categorical_accuracy: 0.8009\n",
      "Epoch 19/20\n",
      "101/101 [==============================] - 68s 672ms/step - loss: 0.7034 - categorical_accuracy: 0.8252\n",
      "Epoch 20/20\n",
      "101/101 [==============================] - 66s 650ms/step - loss: 0.6229 - categorical_accuracy: 0.8457\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:2'):\n",
    "    md.fit_generator(\n",
    "    generator(sequences[:10000], next_words[:10000], BATCH_GEN),\n",
    "    steps_per_epoch=int(len(sequences[:10000])/BATCH_GEN) + 1,\n",
    "    epochs=num_epochs,\n",
    "    callbacks=callbacks_list,\n",
    "    #validation_split=0.02,\n",
    "    #shuffle=True,\n",
    "    #validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
    "    #validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gGn31PVL9Sb"
   },
   "outputs": [],
   "source": [
    "#save the model\n",
    "md.save(os.path.join(save_dir,  'my_model_gen_sentences_lstm2.final.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vb770U0-L9Sj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocabulary...\n"
     ]
    }
   ],
   "source": [
    "#load vocabulary\n",
    "print(\"loading vocabulary...\")\n",
    "# vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
    "\n",
    "with open(vocab_file, 'rb') as f:\n",
    "        words, vocab, vocabulary_inv = cPickle.load(f)\n",
    "\n",
    "vocab_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KzcfeWZUL9Sq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "# load the model\n",
    "print(\"loading model...\")\n",
    "model = load_model(save_dir + \"/\" + 'my_model_gen_sentences_lstm2.final.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UY6hpEXtL9S3"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uwelZdCtL9TC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with the following seed: \". . . . . . . para la tarea\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initiate sentences\n",
    "#seed_sentences = \"estoy muy cansado para estudiar hoy\"\n",
    "#seed_sentences = \"todos vamos a\"\n",
    "seed_sentences = \"para la tarea\"\n",
    "generated = ''\n",
    "sentence = []\n",
    "for i in range (seq_length):\n",
    "    sentence.append(\".\")\n",
    "\n",
    "seed = seed_sentences.split()\n",
    "\n",
    "for i in range(len(seed)):\n",
    "    sentence[seq_length-i-1]=seed[len(seed)-i-1]\n",
    "\n",
    "generated += ' '.join(sentence)\n",
    "print('Generating text with the following seed: \"' + ' '.join(sentence) + '\"')\n",
    "\n",
    "print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMsI0aX-L9TL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\opt\\Anaconda3\\envs\\SI\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . . para la tarea me desperte a dormir para que ir a la nina que aun voy a dormir un ganas de irme a\n"
     ]
    }
   ],
   "source": [
    "words_number = 20\n",
    "#generate the text\n",
    "for i in range(words_number):\n",
    "    #create the vector\n",
    "    x = np.zeros((1, seq_length, vocab_size))\n",
    "    for t, word in enumerate(sentence):\n",
    "        x[0, t, vocab[word]] = 1.\n",
    "    #print(x.shape)\n",
    "\n",
    "    #calculate next word\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, 0.34)\n",
    "    next_word = vocabulary_inv[next_index]\n",
    "\n",
    "    #add the next word to the text\n",
    "    generated += \" \" + next_word\n",
    "    # shift the sentence by one, and and the next word at its end\n",
    "    sentence = sentence[1:] + [next_word]\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-uuJp8qCL9TU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "lstm-bi-text-generation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
