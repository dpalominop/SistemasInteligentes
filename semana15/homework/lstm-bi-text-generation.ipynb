{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm-bi-text-generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpalominop/SistemasInteligentes/blob/master/semana15/homework/lstm-bi-text-generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_qFRwvLVL9Qs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test Generation using a Bidirectional LSTM Network"
      ]
    },
    {
      "metadata": {
        "id": "B1Eqhf6JL9Qy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br/>"
      ]
    },
    {
      "metadata": {
        "id": "dWcJZBNJL9Q1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "159620e6-bf9e-47c8-9f5d-fd74a82d0401"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "#import Keras library\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import LSTM, Input, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.metrics import categorical_accuracy\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "print(K.tensorflow_backend._get_available_gpus())\n",
        "config = tf.ConfigProto(intra_op_parallelism_threads=4, \\\n",
        "                        inter_op_parallelism_threads=4, \\\n",
        "                        allow_soft_placement=True,\\\n",
        "                        device_count = {'CPU' : 1, 'GPU' : 1})\n",
        "tf.device('/gpu:1')\n",
        "sess = tf.Session(config=config)\n",
        "K.set_session(sess)\n",
        "\n",
        "#import spacy, and spacy french model\n",
        "# spacy is used to work on text\n",
        "import spacy\n",
        "!python -m spacy download es\n",
        "nlp = spacy.load('es')\n",
        "\n",
        "#import other libraries\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import codecs\n",
        "import collections\n",
        "from six.moves import cPickle\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "Requirement already satisfied: es_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.0.0/es_core_news_sm-2.0.0.tar.gz#egg=es_core_news_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/es_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/es\n",
            "\n",
            "    You can now load the model via spacy.load('es')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bZ4qMpiRMSOs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v77u-lhBL9RF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#define parameters used in the tutorial\n",
        "data_dir = 'data/'# data directory containing raw texts\n",
        "save_dir = 'vocab' # directory to store trained NN models\n",
        "file_list = [\"spanish_emojis\"]\n",
        "\n",
        "link = \"https://drive.google.com/open?id=1kdV9lTy7RNp4xYX6_IMA-6wfMRyXNSgv\"\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('words_vocab.pkl')\n",
        "vocab_file = \"words_vocab.pkl\" #os.path.join(save_dir, \"words_vocab.pkl\")\n",
        "\n",
        "link = \"https://drive.google.com/open?id=1xr0ZW6lBDTALEqaAdZefu-DwgEPIgM83\"\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('words_list.pkl')\n",
        "words_list_file = \"words_list.pkl\" #os.path.join(save_dir, \"words_list.pkl\")\n",
        "\n",
        "link = \"https://drive.google.com/open?id=17o5LV6AizHqPw9QeIjKwFXmTASpNs673\"\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('spanish_emojis.csv')\n",
        "data_file = \"spanish_emojis.csv\" #os.path.join(data_dir, file_list[0] + \".csv\")\n",
        "\n",
        "sequences_step = 1 #step to create sequences\n",
        "seq_length = 10 # sequence length\n",
        "preprocess = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "herJ9AX3L9RO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_file(filename):\n",
        "    items = []\n",
        "    if os.path.exists(filename):\n",
        "        # try:\n",
        "        with open(filename, 'rb') as fname:\n",
        "            while True:\n",
        "                try:\n",
        "                    items = cPickle.load(fname)\n",
        "                except EOFError:\n",
        "                    print(EOFError)\n",
        "                    break\n",
        "    else:\n",
        "        items = []\n",
        "    \n",
        "    return items"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CaMf9GfIL9RU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_wordlist(doc):\n",
        "    wl = []\n",
        "    for word in doc:\n",
        "        if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
        "            wl.append(word.text.lower())\n",
        "    return wl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lmDYsfBLL9RZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if(preprocess):\n",
        "    wordlist = []\n",
        "    input_file = pd.read_csv(data_file)\n",
        "    lines = input_file['observations']\n",
        "    for data in lines:\n",
        "        #create sentences\n",
        "        doc = nlp(data)\n",
        "        wl = create_wordlist(doc)\n",
        "        wordlist = wordlist + wl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VYPBqN0JL9Rf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if(preprocess):\n",
        "    # count the number of words\n",
        "    word_counts = collections.Counter(wordlist)\n",
        "\n",
        "    # Mapping from index to word : that's the vocabulary\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
        "\n",
        "    # Mapping from word to index\n",
        "    vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    words = [x[0] for x in word_counts.most_common()]\n",
        "\n",
        "    #size of the vocabulary\n",
        "    vocab_size = len(words)\n",
        "    print(\"vocab size: \", vocab_size)\n",
        "\n",
        "    #save the words and vocabulary\n",
        "    with open(os.path.join(vocab_file), 'wb') as f:\n",
        "        cPickle.dump((words, vocab, vocabulary_inv), f)\n",
        "    with open(os.path.join(words_list_file), 'wb') as f:\n",
        "        cPickle.dump((wordlist), f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l--bcrWRL9Rl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "e3a22cf7-cbbc-4813-c77c-aa0291259fbd"
      },
      "cell_type": "code",
      "source": [
        "if(not preprocess):\n",
        "    wordlist = read_file(words_list_file)\n",
        "    (words, vocab, vocabulary_inv) = read_file(vocab_file)\n",
        "    \n",
        "    # count the number of words\n",
        "    word_counts = collections.Counter(wordlist)\n",
        "    \n",
        "    #size of the vocabulary\n",
        "    vocab_size = len(words)\n",
        "    print(\"vocab size: \", vocab_size)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'EOFError'>\n",
            "<class 'EOFError'>\n",
            "vocab size:  66846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4rN3yd8QL9Rv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "89b8c14f-74fa-450c-9c56-efd172f04c8a"
      },
      "cell_type": "code",
      "source": [
        "#create sequences\n",
        "sequences = []\n",
        "next_words = []\n",
        "for i in range(0, len(wordlist) - seq_length, sequences_step):\n",
        "    sequences.append(wordlist[i: i + seq_length])\n",
        "    next_words.append(wordlist[i + seq_length])\n",
        "\n",
        "print('nb sequences:', len(sequences))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb sequences: 1443918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-iEN3bNgL9R_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bidirectional_lstm_model(seq_length, vocab_size):\n",
        "    print('Build LSTM model.')\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(vocab_size))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    optimizer = Adam(lr=learning_rate)\n",
        "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5akgUQYLL9SI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_size = 256 # size of RNN\n",
        "batch_size = 64 # minibatch size\n",
        "seq_length = 10 # sequence length\n",
        "num_epochs = 10 # number of epochs\n",
        "learning_rate = 0.001 #learning rate\n",
        "sequences_step = 1 #step to create sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cYYiskZQL9SM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "edd02b63-56b7-44bc-c876-740c8a2d5853"
      },
      "cell_type": "code",
      "source": [
        "md = bidirectional_lstm_model(seq_length, vocab_size)\n",
        "md.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build LSTM model.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_1 (Bidirection (None, 512)               137426944 \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 66846)             34291998  \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 66846)             0         \n",
            "=================================================================\n",
            "Total params: 171,718,942\n",
            "Trainable params: 171,718,942\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OZr1pPJWL9SW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "outputId": "8231fef4-d403-4126-8cdc-72ebc2f0f307"
      },
      "cell_type": "code",
      "source": [
        "#X = np.zeros((len(sequences)//500, seq_length, vocab_size), dtype=np.bool)\n",
        "#y = np.zeros((len(sequences)//500, vocab_size), dtype=np.bool)\n",
        "X = np.zeros((1000, seq_length, vocab_size), dtype=np.bool)\n",
        "y = np.zeros((1000, vocab_size), dtype=np.bool)\n",
        "n=0\n",
        "for i, sentence in enumerate(sequences[1000*n:1000*(n+1)]):\n",
        "    for t, word in enumerate(sentence):\n",
        "        X[i, t, vocab[word]] = 1\n",
        "    y[i, vocab[next_words[i]]] = 1\n",
        "\n",
        "#fit the model\n",
        "callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
        "           ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_gen_sentences_lstm.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n",
        "                           monitor='val_loss', verbose=0, mode='auto', period=2)]\n",
        "history = md.fit(X, y,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True,\n",
        "                 epochs=num_epochs,\n",
        "                 callbacks=callbacks,\n",
        "                 validation_split=0.01)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 990 samples, validate on 10 samples\n",
            "Epoch 1/10\n",
            "990/990 [==============================] - 256s 259ms/step - loss: 11.0935 - categorical_accuracy: 0.0283 - val_loss: 10.9878 - val_categorical_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "320/990 [========>.....................] - ETA: 2:39 - loss: 10.6547 - categorical_accuracy: 0.0406"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-13f41fcd7ce0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                  \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                  validation_split=0.01)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4gGn31PVL9Sb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "md.save(save_dir + \"/\" + 'my_model_gen_sentences_lstm.final.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vb770U0-L9Sj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load vocabulary\n",
        "print(\"loading vocabulary...\")\n",
        "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
        "\n",
        "with open(os.path.join(save_dir, 'words_vocab.pkl'), 'rb') as f:\n",
        "        words, vocab, vocabulary_inv = cPickle.load(f)\n",
        "\n",
        "vocab_size = len(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KzcfeWZUL9Sq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "# load the model\n",
        "print(\"loading model...\")\n",
        "model = load_model(save_dir + \"/\" + 'my_model_gen_sentences_lstm.final.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UY6hpEXtL9S3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uwelZdCtL9TC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#initiate sentences\n",
        "seed_sentences = \"estoy muy cansado para estudiar .\"\n",
        "generated = ''\n",
        "sentence = []\n",
        "for i in range (seq_length):\n",
        "    sentence.append(\"a\")\n",
        "\n",
        "seed = seed_sentences.split()\n",
        "\n",
        "for i in range(len(seed)):\n",
        "    sentence[seq_length-i-1]=seed[len(seed)-i-1]\n",
        "\n",
        "generated += ' '.join(sentence)\n",
        "print('Generating text with the following seed: \"' + ' '.join(sentence) + '\"')\n",
        "\n",
        "print ()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AMsI0aX-L9TL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words_number = 10\n",
        "#generate the text\n",
        "for i in range(words_number):\n",
        "    #create the vector\n",
        "    x = np.zeros((1, seq_length, vocab_size))\n",
        "    for t, word in enumerate(sentence):\n",
        "        x[0, t, vocab[word]] = 1.\n",
        "    #print(x.shape)\n",
        "\n",
        "    #calculate next word\n",
        "    preds = model.predict(x, verbose=0)[0]\n",
        "    next_index = sample(preds, 0.34)\n",
        "    next_word = vocabulary_inv[next_index]\n",
        "\n",
        "    #add the next word to the text\n",
        "    generated += \" \" + next_word\n",
        "    # shift the sentence by one, and and the next word at its end\n",
        "    sentence = sentence[1:] + [next_word]\n",
        "\n",
        "print(generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-uuJp8qCL9TU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}