{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Generation using a Bidirectional LSTM Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#import Keras library\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Input, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import categorical_accuracy\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4, \\\n",
    "                        inter_op_parallelism_threads=4, \\\n",
    "                        allow_soft_placement=True,\\\n",
    "                        device_count = {'CPU' : 1, 'GPU' : 1})\n",
    "tf.device('/gpu:1')\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "#import spacy, and spacy french model\n",
    "# spacy is used to work on text\n",
    "import spacy\n",
    "# !python -m spacy download es\n",
    "nlp = spacy.load('es')\n",
    "\n",
    "#import other libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameters used in the tutorial\n",
    "data_dir = 'data/'# data directory containing raw texts\n",
    "save_dir = 'vocab' # directory to store trained NN models\n",
    "file_list = [\"spanish_emojis\"]\n",
    "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
    "words_list_file = os.path.join(save_dir, \"words_list.pkl\")\n",
    "sequences_step = 1 #step to create sequences\n",
    "seq_length = 10 # sequence length\n",
    "preprocess = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    items = []\n",
    "    if os.path.exists(filename):\n",
    "        # try:\n",
    "        with open(filename, 'rb') as fname:\n",
    "            while True:\n",
    "                try:\n",
    "                    items = cPickle.load(fname)\n",
    "                except EOFError:\n",
    "                    print(EOFError)\n",
    "                    break\n",
    "    else:\n",
    "        items = []\n",
    "    \n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordlist(doc):\n",
    "    wl = []\n",
    "    for word in doc:\n",
    "        if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "            wl.append(word.text.lower())\n",
    "    return wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(preprocess):\n",
    "    wordlist = []\n",
    "    input_file = pd.read_csv(os.path.join(data_dir, file_list[0] + \".csv\"))\n",
    "    lines = input_file['observations']\n",
    "    for data in lines:\n",
    "        #create sentences\n",
    "        doc = nlp(data)\n",
    "        wl = create_wordlist(doc)\n",
    "        wordlist = wordlist + wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(preprocess):\n",
    "    # count the number of words\n",
    "    word_counts = collections.Counter(wordlist)\n",
    "\n",
    "    # Mapping from index to word : that's the vocabulary\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "\n",
    "    # Mapping from word to index\n",
    "    vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    words = [x[0] for x in word_counts.most_common()]\n",
    "\n",
    "    #size of the vocabulary\n",
    "    vocab_size = len(words)\n",
    "    print(\"vocab size: \", vocab_size)\n",
    "\n",
    "    #save the words and vocabulary\n",
    "    with open(os.path.join(vocab_file), 'wb') as f:\n",
    "        cPickle.dump((words, vocab, vocabulary_inv), f)\n",
    "    with open(os.path.join(words_list_file), 'wb') as f:\n",
    "        cPickle.dump((wordlist), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'EOFError'>\n",
      "<class 'EOFError'>\n",
      "vocab size:  66846\n"
     ]
    }
   ],
   "source": [
    "if(not preprocess):\n",
    "    wordlist = read_file(words_list_file)\n",
    "    (words, vocab, vocabulary_inv) = read_file(vocab_file)\n",
    "    \n",
    "    # count the number of words\n",
    "    word_counts = collections.Counter(wordlist)\n",
    "    \n",
    "    #size of the vocabulary\n",
    "    vocab_size = len(words)\n",
    "    print(\"vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 1443918\n"
     ]
    }
   ],
   "source": [
    "#create sequences\n",
    "sequences = []\n",
    "next_words = []\n",
    "for i in range(0, len(wordlist) - seq_length, sequences_step):\n",
    "    sequences.append(wordlist[i: i + seq_length])\n",
    "    next_words.append(wordlist[i + seq_length])\n",
    "\n",
    "print('nb sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_lstm_model(seq_length, vocab_size):\n",
    "    print('Build LSTM model.')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_size = 256 # size of RNN\n",
    "batch_size = 64 # minibatch size\n",
    "seq_length = 10 # sequence length\n",
    "num_epochs = 10 # number of epochs\n",
    "learning_rate = 0.001 #learning rate\n",
    "sequences_step = 1 #step to create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 512)               137426944 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 66846)             34291998  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 66846)             0         \n",
      "=================================================================\n",
      "Total params: 171,718,942\n",
      "Trainable params: 171,718,942\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "md = bidirectional_lstm_model(seq_length, vocab_size)\n",
    "md.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 990 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "990/990 [==============================] - 160s 161ms/step - loss: 11.0954 - categorical_accuracy: 0.0343 - val_loss: 11.0207 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "990/990 [==============================] - 143s 145ms/step - loss: 8.8987 - categorical_accuracy: 0.0404 - val_loss: 7.9525 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "990/990 [==============================] - 141s 142ms/step - loss: 6.6163 - categorical_accuracy: 0.0384 - val_loss: 7.8613 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "990/990 [==============================] - 140s 141ms/step - loss: 6.1966 - categorical_accuracy: 0.0364 - val_loss: 7.8530 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "990/990 [==============================] - 148s 149ms/step - loss: 5.9673 - categorical_accuracy: 0.0414 - val_loss: 7.7858 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "990/990 [==============================] - 161s 162ms/step - loss: 5.7884 - categorical_accuracy: 0.0374 - val_loss: 7.9950 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "990/990 [==============================] - 153s 154ms/step - loss: 5.6302 - categorical_accuracy: 0.0343 - val_loss: 8.1884 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "990/990 [==============================] - 157s 159ms/step - loss: 5.5229 - categorical_accuracy: 0.0424 - val_loss: 8.2354 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "990/990 [==============================] - 153s 155ms/step - loss: 5.4663 - categorical_accuracy: 0.0343 - val_loss: 8.2529 - val_categorical_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "#X = np.zeros((len(sequences)//500, seq_length, vocab_size), dtype=np.bool)\n",
    "#y = np.zeros((len(sequences)//500, vocab_size), dtype=np.bool)\n",
    "X = np.zeros((1000, seq_length, vocab_size), dtype=np.bool)\n",
    "y = np.zeros((1000, vocab_size), dtype=np.bool)\n",
    "n=0\n",
    "for i, sentence in enumerate(sequences[1000*n:1000*(n+1)]):\n",
    "    for t, word in enumerate(sentence):\n",
    "        X[i, t, vocab[word]] = 1\n",
    "    y[i, vocab[next_words[i]]] = 1\n",
    "\n",
    "#fit the model\n",
    "callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_gen_sentences_lstm.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n",
    "                           monitor='val_loss', verbose=0, mode='auto', period=2)]\n",
    "history = md.fit(X, y,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 epochs=num_epochs,\n",
    "                 callbacks=callbacks,\n",
    "                 validation_split=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "md.save(save_dir + \"/\" + 'my_model_gen_sentences_lstm.final.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocabulary...\n"
     ]
    }
   ],
   "source": [
    "#load vocabulary\n",
    "print(\"loading vocabulary...\")\n",
    "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
    "\n",
    "with open(os.path.join(save_dir, 'words_vocab.pkl'), 'rb') as f:\n",
    "        words, vocab, vocabulary_inv = cPickle.load(f)\n",
    "\n",
    "vocab_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "# load the model\n",
    "print(\"loading model...\")\n",
    "model = load_model(save_dir + \"/\" + 'my_model_gen_sentences_lstm.final.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with the following seed: \"a a a a estoy muy cansado para estudiar .\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initiate sentences\n",
    "seed_sentences = \"estoy muy cansado para estudiar .\"\n",
    "generated = ''\n",
    "sentence = []\n",
    "for i in range (seq_length):\n",
    "    sentence.append(\"a\")\n",
    "\n",
    "seed = seed_sentences.split()\n",
    "\n",
    "for i in range(len(seed)):\n",
    "    sentence[seq_length-i-1]=seed[len(seed)-i-1]\n",
    "\n",
    "generated += ' '.join(sentence)\n",
    "print('Generating text with the following seed: \"' + ' '.join(sentence) + '\"')\n",
    "\n",
    "print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a a a estoy muy cansado para estudiar . de me mal a no de me mi a entre\n"
     ]
    }
   ],
   "source": [
    "words_number = 10\n",
    "#generate the text\n",
    "for i in range(words_number):\n",
    "    #create the vector\n",
    "    x = np.zeros((1, seq_length, vocab_size))\n",
    "    for t, word in enumerate(sentence):\n",
    "        x[0, t, vocab[word]] = 1.\n",
    "    #print(x.shape)\n",
    "\n",
    "    #calculate next word\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, 0.34)\n",
    "    next_word = vocabulary_inv[next_index]\n",
    "\n",
    "    #add the next word to the text\n",
    "    generated += \" \" + next_word\n",
    "    # shift the sentence by one, and and the next word at its end\n",
    "    sentence = sentence[1:] + [next_word]\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
