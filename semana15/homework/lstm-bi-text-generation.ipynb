{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm-bi-text-generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpalominop/SistemasInteligentes/blob/master/semana15/homework/lstm-bi-text-generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_qFRwvLVL9Qs"
      },
      "cell_type": "markdown",
      "source": [
        "# Test Generation using a Bidirectional LSTM Network"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "B1Eqhf6JL9Qy"
      },
      "cell_type": "markdown",
      "source": [
        "<br/>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dWcJZBNJL9Q1",
        "outputId": "c3d18967-09d0-40e4-cfda-a1650e6f26ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "#import Keras library\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import LSTM, Input, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, LambdaCallback\n",
        "from keras.metrics import categorical_accuracy\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "print(K.tensorflow_backend._get_available_gpus())\n",
        "config = tf.ConfigProto(intra_op_parallelism_threads=4, \\\n",
        "                        inter_op_parallelism_threads=4, \\\n",
        "                        allow_soft_placement=True,\\\n",
        "                        device_count = {'CPU' : 1, 'GPU' : 1})\n",
        "tf.device('/gpu:1')\n",
        "sess = tf.Session(config=config)\n",
        "K.set_session(sess)\n",
        "\n",
        "#import spacy, and spacy french model\n",
        "# spacy is used to work on text\n",
        "!pip install spacy\n",
        "import spacy\n",
        "!python -m spacy download es\n",
        "nlp = spacy.load('es')\n",
        "\n",
        "#import other libraries\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import codecs\n",
        "import collections\n",
        "from six.moves import cPickle\n",
        "import pandas as pd"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.0)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.9)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.1)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.28.1)\n",
            "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
            "Requirement already satisfied: es_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.0.0/es_core_news_sm-2.0.0.tar.gz#egg=es_core_news_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/es_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/es\n",
            "\n",
            "    You can now load the model via spacy.load('es')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bZ4qMpiRMSOs",
        "outputId": "fcf22f50-6b5a-406c-85d4-fba8523d75dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "gdrive = True\n",
        "if gdrive:\n",
        "    !pip install -U -q PyDrive\n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "\n",
        "    # Authenticate and create the PyDrive client.\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    \n",
        "    !curl https://raw.githubusercontent.com/dexterfichuk/GoogleDriveCheckpoint/master/google_drive_checkpoint.py -O\n",
        "      \n",
        "    from google_drive_checkpoint import GoogleDriveCheckpoint"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 20.4MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 3.4MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 4.8MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 3.1MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 3.8MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 4.5MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 5.1MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 5.8MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 6.4MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 5.0MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 5.1MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 6.9MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 6.9MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 12.4MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 12.6MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 12.5MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 12.7MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 12.8MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 12.9MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 42.0MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 14.9MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 14.8MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 15.1MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 14.9MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 14.9MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 14.3MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 14.4MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 14.4MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 14.3MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 14.9MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 44.1MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 46.8MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 48.3MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 45.1MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 45.3MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 53.9MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 19.6MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 19.7MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 19.4MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 19.2MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 19.2MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 18.9MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 18.7MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 18.9MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 18.8MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 18.7MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 46.8MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 46.1MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 48.5MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 45.9MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 46.6MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 48.9MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 49.3MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 55.0MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 56.4MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 57.8MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 58.9MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 61.1MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 61.9MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 67.9MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 67.7MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 68.0MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 68.7MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 67.5MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 50.0MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 49.6MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 49.6MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 50.0MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 49.8MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 49.7MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 34.5MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 33.6MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 33.3MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 33.1MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 39.4MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 38.7MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 38.0MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 37.9MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 37.3MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 37.6MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 57.1MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 58.5MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 59.9MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 52.7MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 52.9MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 54.8MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 56.1MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 56.3MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 58.2MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 58.1MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 58.2MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 59.4MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 58.2MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 69.7MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 70.1MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 69.9MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 21.9MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  4522  100  4522    0     0  25262      0 --:--:-- --:--:-- --:--:-- 25262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "v77u-lhBL9RF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#define parameters used in the tutorial\n",
        "data_dir = 'data/'# data directory containing raw texts\n",
        "save_dir = 'vocab' # directory to store trained NN models\n",
        "file_list = [\"spanish_emojis\"]\n",
        "\n",
        "if gdrive:\n",
        "    link = \"https://drive.google.com/open?id=1kdV9lTy7RNp4xYX6_IMA-6wfMRyXNSgv\"\n",
        "    fluff, id = link.split('=')\n",
        "    downloaded = drive.CreateFile({'id':id}) \n",
        "    downloaded.GetContentFile('words_vocab.pkl')\n",
        "    vocab_file = \"words_vocab.pkl\" \n",
        "    \n",
        "    link = \"https://drive.google.com/open?id=1xr0ZW6lBDTALEqaAdZefu-DwgEPIgM83\"\n",
        "    fluff, id = link.split('=')\n",
        "    downloaded = drive.CreateFile({'id':id}) \n",
        "    downloaded.GetContentFile('words_list.pkl')\n",
        "    words_list_file = \"words_list.pkl\" \n",
        "    \n",
        "    link = \"https://drive.google.com/open?id=17o5LV6AizHqPw9QeIjKwFXmTASpNs673\"\n",
        "    fluff, id = link.split('=')\n",
        "    downloaded = drive.CreateFile({'id':id}) \n",
        "    downloaded.GetContentFile('spanish_emojis.csv')\n",
        "    data_file = \"spanish_emojis.csv\"\n",
        "    \n",
        "    link = \"https://drive.google.com/open?id=1gTUqnhuknwFP4BIFQixRIjB7V-CQyb_r\"\n",
        "    fluff, id = link.split('=')\n",
        "    downloaded = drive.CreateFile({'id':id}) \n",
        "    downloaded.GetContentFile('sequences_list.pkl')\n",
        "    sequences_file = \"sequences_list.pkl\" \n",
        "    \n",
        "    link = \"https://drive.google.com/open?id=13NEQdiBBn-3vCy8Q5JF-OBzrD7hs680j\"\n",
        "    fluff, id = link.split('=')\n",
        "    downloaded = drive.CreateFile({'id':id}) \n",
        "    downloaded.GetContentFile('next_words_list.pkl')\n",
        "    next_words_file = \"next_words_list.pkl\"\n",
        "    \n",
        "else:   \n",
        "    vocab_file = os.path.join(data_dir, \"words_vocab.pkl\")\n",
        "    words_list_file = os.path.join(data_dir, \"words_list.pkl\")\n",
        "    data_file = os.path.join(data_dir, file_list[0] + \".csv\")\n",
        "    sequences_file = os.path.join(data_dir, \"sequences_list.pkl\")\n",
        "    next_words_file = os.path.join(data_dir, \"next_words_list.pkl\")\n",
        "\n",
        "sequences_step = 1 #step to create sequences\n",
        "seq_length = 10 # sequence length\n",
        "preprocess = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "herJ9AX3L9RO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_file(filename):\n",
        "    items = []\n",
        "    if os.path.exists(filename):\n",
        "        # try:\n",
        "        with open(filename, 'rb') as fname:\n",
        "            while True:\n",
        "                try:\n",
        "                    items = cPickle.load(fname)\n",
        "                except EOFError:\n",
        "                    print(EOFError)\n",
        "                    break\n",
        "    else:\n",
        "        items = []\n",
        "    \n",
        "    return items"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CaMf9GfIL9RU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_wordlist(doc):\n",
        "    wl = []\n",
        "    for word in doc:\n",
        "        if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
        "            wl.append(word.text.lower())\n",
        "    return wl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lmDYsfBLL9RZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequences = []\n",
        "next_words = []\n",
        "wordlist = []\n",
        "if(preprocess):\n",
        "    input_file = pd.read_csv(data_file)\n",
        "    lines = input_file['observations']\n",
        "    for data in lines:\n",
        "        #create sentences\n",
        "        doc = nlp(data)\n",
        "        wl = create_wordlist(doc)\n",
        "        wordlist = wordlist + wl\n",
        "        if(len(wl) < 2): \n",
        "            continue\n",
        "        seq = [' ' for i in range(seq_length - len(wl) + 1)]\n",
        "        seq = seq + wl\n",
        "\n",
        "        for i in range(0, len(seq) - seq_length, sequences_step):\n",
        "          sequences.append(seq[i: i + seq_length])\n",
        "          next_words.append(seq[i + seq_length])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VYPBqN0JL9Rf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if(preprocess):\n",
        "    # count the number of words\n",
        "    word_counts = collections.Counter(wordlist)\n",
        "\n",
        "    # Mapping from index to word : that's the vocabulary\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
        "\n",
        "    # Mapping from word to index\n",
        "    vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    words = [x[0] for x in word_counts.most_common()]\n",
        "\n",
        "    #size of the vocabulary\n",
        "    vocab_size = len(words)\n",
        "    print(\"vocab size: \", vocab_size)\n",
        "\n",
        "    #save the words and vocabulary\n",
        "    with open(os.path.join(vocab_file), 'wb') as f:\n",
        "        cPickle.dump((words, vocab, vocabulary_inv), f)\n",
        "    with open(os.path.join(words_list_file), 'wb') as f:\n",
        "        cPickle.dump((wordlist), f)\n",
        "    with open(os.path.join(sequences_file), 'wb') as f:\n",
        "        cPickle.dump((sequences), f)\n",
        "    with open(os.path.join(next_words_file), 'wb') as f:\n",
        "        cPickle.dump((next_words), f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "l--bcrWRL9Rl",
        "outputId": "7105c456-3238-418b-9560-ecd1c175ab66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "if(not preprocess):\n",
        "    wordlist = read_file(words_list_file)\n",
        "    (words, vocab, vocabulary_inv) = read_file(vocab_file)\n",
        "    sequences = read_file(sequences_file)\n",
        "    next_words = read_file(next_words_file)\n",
        "    \n",
        "    # count the number of words\n",
        "    word_counts = collections.Counter(wordlist)\n",
        "    \n",
        "    #size of the vocabulary\n",
        "    vocab_size = len(words)\n",
        "    print(\"vocab size: \", vocab_size)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'EOFError'>\n",
            "<class 'EOFError'>\n",
            "<class 'EOFError'>\n",
            "<class 'EOFError'>\n",
            "vocab size:  66846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4rN3yd8QL9Rv",
        "outputId": "4c6013f5-10b9-45cd-dfb9-ef7dccb91d46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "#create sequences\n",
        "#sequences = []\n",
        "#next_words = []\n",
        "#for i in range(0, len(wordlist) - seq_length, sequences_step):\n",
        "#    sequences.append(wordlist[i: i + seq_length])\n",
        "#    next_words.append(wordlist[i + seq_length])\n",
        "\n",
        "print('nb sequences: ', len(sequences))\n",
        "print('vocav( ): ', vocab[' '])\n",
        "print('words(0): ', vocabulary_inv[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb sequences:  421621\n",
            "vocav( ):  0\n",
            "words(0):   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-iEN3bNgL9R_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bidirectional_lstm_model(seq_length, vocab_size):\n",
        "    print('Build LSTM model.')\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(vocab_size))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    optimizer = Adam(lr=learning_rate)\n",
        "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5akgUQYLL9SI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_size = 256 # size of RNN\n",
        "batch_size = 64 # minibatch size\n",
        "seq_length = 10 # sequence length\n",
        "num_epochs = 10 # number of epochs\n",
        "learning_rate = 0.001 #learning rate\n",
        "sequences_step = 1 #step to create sequences\n",
        "BATCH_GEN = 2000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cYYiskZQL9SM",
        "outputId": "cf9375ea-59c1-438b-b3d0-2f0b8ea55b52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "cell_type": "code",
      "source": [
        "md = bidirectional_lstm_model(seq_length, vocab_size)\n",
        "md.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build LSTM model.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_1 (Bidirection (None, 512)               137426944 \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 66846)             34291998  \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 66846)             0         \n",
            "=================================================================\n",
            "Total params: 171,718,942\n",
            "Trainable params: 171,718,942\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S6RDgEznwzVE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator(sentence_list, next_word_list, batch_size):\n",
        "    index = 0\n",
        "    while True:\n",
        "        x = np.zeros((batch_size, seq_length, len(words)), dtype=np.bool)\n",
        "        y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
        "        for i in range(batch_size):\n",
        "            for t, w in enumerate(sentence_list[index]):\n",
        "                x[i, t, vocab[w]] = 1\n",
        "            y[i, vocab[next_words[index]]] = 1\n",
        "\n",
        "            index = index + 1\n",
        "            if index == len(sentence_list):\n",
        "                index = 0\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LBvSvUgjwzVI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_path = \"BI-LSTM-epoch{epoch:03d}-words%d-sequence%d-loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % (\n",
        "    len(words),\n",
        "    seq_length\n",
        ")\n",
        "#checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
        "checkpoint = GoogleDriveCheckpoint(file_path, drive, save_best_only=True, save_weights_only=True, verbose=1, mode='auto')\n",
        "\n",
        "#print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
        "callbacks_list = [checkpoint, \n",
        "                  #print_callback, \n",
        "                  early_stopping]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CQxZQxwQwzVN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1455
        },
        "outputId": "305f8146-92ae-4cb5-d567-8aff782319e7"
      },
      "cell_type": "code",
      "source": [
        "md.fit_generator(\n",
        "    generator(sequences, next_words, BATCH_GEN),\n",
        "    steps_per_epoch=int(len(sequences)/BATCH_GEN) + 1,\n",
        "    epochs=num_epochs,\n",
        "    callbacks=callbacks_list,\n",
        "    #validation_split=0.02,\n",
        "    #shuffle=True,\n",
        "    #validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
        "    #validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n",
        "    )"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d36dcae6dbce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mBATCH_GEN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#validation_split=0.02,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#shuffle=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     \u001b[0;34m\"`use_multiprocessing=False, workers > 1`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                     \"For more information see issue #1638.\")\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mnext_sample\u001b[0;34m(uid)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0muid\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \"\"\"\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-2871a41bcaf8>\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(sentence_list, next_word_list, batch_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_word_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_indices' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4gGn31PVL9Sb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "]#save the model\n",
        "md.save(save_dir + \"/\" + 'my_model_gen_sentences_lstm.final.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vb770U0-L9Sj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load vocabulary\n",
        "print(\"loading vocabulary...\")\n",
        "# vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
        "\n",
        "with open(vocab_file, 'rb') as f:\n",
        "        words, vocab, vocabulary_inv = cPickle.load(f)\n",
        "\n",
        "vocab_size = len(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KzcfeWZUL9Sq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "# load the model\n",
        "print(\"loading model...\")\n",
        "model = load_model(save_dir + \"/\" + 'my_model_gen_sentences_lstm.final.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UY6hpEXtL9S3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uwelZdCtL9TC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#initiate sentences\n",
        "#seed_sentences = \"estoy muy cansado para estudiar hoy\"\n",
        "seed_sentences = \"como estas\"\n",
        "generated = ''\n",
        "sentence = []\n",
        "for i in range (seq_length):\n",
        "    sentence.append(\".\")\n",
        "\n",
        "seed = seed_sentences.split()\n",
        "\n",
        "for i in range(len(seed)):\n",
        "    sentence[seq_length-i-1]=seed[len(seed)-i-1]\n",
        "\n",
        "generated += ' '.join(sentence)\n",
        "print('Generating text with the following seed: \"' + ' '.join(sentence) + '\"')\n",
        "\n",
        "print ()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "AMsI0aX-L9TL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words_number = 10\n",
        "#generate the text\n",
        "for i in range(words_number):\n",
        "    #create the vector\n",
        "    x = np.zeros((1, seq_length, vocab_size))\n",
        "    for t, word in enumerate(sentence):\n",
        "        x[0, t, vocab[word]] = 1.\n",
        "    #print(x.shape)\n",
        "\n",
        "    #calculate next word\n",
        "    preds = md.predict(x, verbose=0)[0]\n",
        "    next_index = sample(preds, 0.34)\n",
        "    next_word = vocabulary_inv[next_index]\n",
        "\n",
        "    #add the next word to the text\n",
        "    generated += \" \" + next_word\n",
        "    # shift the sentence by one, and and the next word at its end\n",
        "    sentence = sentence[1:] + [next_word]\n",
        "\n",
        "print(generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-uuJp8qCL9TU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}