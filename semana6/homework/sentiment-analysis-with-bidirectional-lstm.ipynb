{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment-analysis-with-bidirectional-lstm.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "_cell_guid": "f2fe8eb4-c597-49e3-8ea6-6bbe2bd522bc",
        "_uuid": "7eafc55c75f801975da61c08cfec5130d315330d",
        "id": "HyBHT0kJjKGJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This kernel is created by [Lilia Simeonova](https://github.com/lilia-simeonova) and it is based on previous work of  [Peter Naggy](https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras/notebook).\n"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "c7614738-4d8d-4d93-832f-859425311be5",
        "_uuid": "03585315a8e4daef0e87c44b6a46d63b90c2e704",
        "id": "VWp40GaHjKGV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are going to review the well-known problem of Sentiment Analysis, but this time we will use the relatively new approach of Deep Learning.\n",
        "\n",
        "Even though some of the most famous applications of the deep neural networks are related to image processing, many recent researches show awesome results resolving nature language processing problems.\n",
        "\n",
        "Recurrent neural networks (which we are about to explore) are a subclass of neural networks, designed to perform a sequences recognition or prediction. They have a flexible number of inputs and they allow cyclical connections between their neurons. This means that they are able to remember previous information and connect it to the current task.\n",
        "\n",
        "Long Short Term Memory networks (LSTM) are a subclass of RNN, specialized in remembering information for a long period of time. More over the Bidirectional lstms keep the contextual information in both directions.\n",
        "\n",
        "Here you can find detailed explanation how LSTM works.\n",
        "\n",
        "In this post we will concentrate on the application part.\n",
        "\n",
        "# Tools\n",
        "Before we start we need to make sure we have the following tools installed:\n",
        "\n",
        "1. Python\n",
        "2. TensorFlow - Google’s open sourced numeric computational library\n",
        "3. Keras - Neural Network Framework, which can run on top of TensorFlow\n",
        "4. Numpy - Package for scientific computations\n",
        "5. Pandas - Package providing easy-to-use data structures and data analysis tools\n",
        "\n",
        "# Approach\n",
        "See below a simple diagram of how we will design our deep neural network."
      ]
    },
    {
      "metadata": {
        "_cell_guid": "b3baa2c9-f033-4c73-b39c-01cbe7e62f2b",
        "_uuid": "aa8ee7b6d04ce24e7a2cd45a89fdb9e1aaaa5a2f",
        "id": "MNTyHSODjKGZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Architecture of our Bidirectional LSTM](http://thelillysblog.com/images/architecture-nn2.jpg)\n"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "1817700c-a5c5-4adb-a384-373fd3c45e3b",
        "_uuid": "97c636523773b91d0d66464a9f0d4f9c879c692d",
        "id": "t0stB1JbjKGe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### For this Kernal I’ve used [Sentiment data](https://www.kaggle.com/sonaam1234/sentimentdata/data) by Sonam Srivastava.\n",
        "\n",
        "# Preprocessing\n",
        "In order to make our life easier we can merge the two files (one with positive and one with negative examples) into one csv file with one column called text and another called sentiment - 1 for positive examples and 0 for negatives. We should also randomize the order.\n",
        "\n",
        "The next thing to do is to prepare our data for the neural network.\n",
        "\n",
        "We can simply use Keras preprocessing methods."
      ]
    },
    {
      "metadata": {
        "_cell_guid": "ddd2321b-d7e7-480b-9b80-9eba9443e6bf",
        "_uuid": "b3d7231a57e0ee9fec8a38f387664daf8c8cdceb",
        "id": "5vO3KAA_jKGi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "54c9734c-f371-4d68-e72b-ca71246a162e"
      },
      "cell_type": "code",
      "source": [
        "#!pip install keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout\n",
        "# from sklearn.model_selection import train_test_split\n",
        "from sklearn.cross_validation  import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/50k_imdb_movie_reviews.csv')\n",
        "#data = pd.read_csv(\"shuffled_movie_data.csv\")\n",
        "\n",
        "tokenizer = Tokenizer(num_words=2000, split=' ')\n",
        "\n",
        "tokenizer.fit_on_texts(data['review'])\n",
        "X = tokenizer.texts_to_sequences(data['review'])\n",
        "X = pad_sequences(X)\n",
        "Y = data['sentiment']\n",
        "\n",
        "# We can then create our train and test sets:\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
        "print('X_train.shape: ', X_train.shape)\n",
        "print('Y_train.shape: ', Y_train.shape)\n",
        "print('X_test.shape: ', X_test.shape)\n",
        "print('Y_test.shape: ', Y_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape:  (40000, 1939)\n",
            "Y_train.shape:  (40000,)\n",
            "X_test.shape:  (10000, 1939)\n",
            "Y_test.shape:  (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "345b71b2-6b7a-47e8-a73b-d56149914abe",
        "_uuid": "9a5a519bd15c8a040be4bed3c01bdf93a3dbaa7e",
        "id": "Iywu6fm7jKGw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In Keras, we can define our deep network as a sequence of layers. As described in the image above, we need to have three layers:\n",
        "\n",
        "1. Embedding Layer - modifies the integer representation of words into dense vectors\n",
        "2. Bidirectional LSTM Layer - connects two hidden layers of opposite directions to the same output\n",
        "3. Dense Layer - output layer with softmax activation"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "c0284f5a-b2bd-4922-91cd-1233a83eade0",
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "_uuid": "b20cefbb00bcb7dd468a8a368e111648c638cee5",
        "id": "vUZn4pItjKG1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "81ccdf92-5856-4e21-c8c4-16c9d640f1b7"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add( Embedding(2000, 32, input_length = X.shape[1], dropout=0.2))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "model.add( Bidirectional( LSTM(100, return_sequences=True)))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "model.add( Bidirectional( LSTM(100)))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "model.add( Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 1939, 32)          64000     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 1939, 32)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_11 (Bidirectio (None, 1939, 200)         106400    \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 1939, 200)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_12 (Bidirectio (None, 200)               240800    \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 411,401\n",
            "Trainable params: 411,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pjMcbmtmoWNu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "211231e0-83a3-4eec-9e42-287ec8221e1f"
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, epochs = 3, batch_size = 64, verbose = 2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WbGubRHSoGyB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0f4a4a6f-5c76-469e-a8b6-55a4d5298188",
        "_uuid": "6f41abb209b60f17dcb6a7a1c632561a5965d763",
        "id": "16A_qr5EjKHE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This deep neural network gives us accuracy of 73%.\n",
        "\n",
        "Note: Feel free to play with the Hyperparameters as much as you like."
      ]
    }
  ]
}