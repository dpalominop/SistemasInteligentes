{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/outofcore_modelpersistence.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDb Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train a simple logistic regression model to classify movie reviews from the 50k IMDb review dataset that has been collected by Maas et. al.\n",
    "\n",
    "> AL Maas, RE Daly, PT Pham, D Huang, AY Ng, and C Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin- guistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics\n",
    "\n",
    "[Source: http://ai.stanford.edu/~amaas/data/sentiment/]\n",
    "\n",
    "The dataset consists of 50,000 movie reviews from the original \"train\" and \"test\" subdirectories. The class labels are binary (1=positive and 0=negative) and contain 25,000 positive and 25,000 negative movie reviews, respectively.\n",
    "For simplicity, I assembled the reviews in a single CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I've expected a comedy about the NVA, but this...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Jamie Foxx was the epitome of Ray Charles. Aft...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>The last couple of weeks in the life of a dead...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>(No need to recap the plot, since others have ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>The movie starts quite with an intriguing scen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  I've expected a comedy about the NVA, but this...          0\n",
       "49996  Jamie Foxx was the epitome of Ray Charles. Aft...          1\n",
       "49997  The last couple of weeks in the life of a dead...          0\n",
       "49998  (No need to recap the plot, since others have ...          1\n",
       "49999  The movie starts quite with an intriguing scen...          0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# if you want to download the original file:\n",
    "#df = pd.read_csv('https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/50k_imdb_movie_reviews.csv')\n",
    "# otherwise load local file\n",
    "#df[['review', 'sentiment']].to_csv('shuffled_movie_data.csv', index=False)\n",
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us shuffle the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>Not worth the video rental or the time or the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>I would recommend this for anyone who is an ad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>This film is about two female killers going on...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>The film is a pathetic attempt to remake Ingma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>What a trip down memory lane.&lt;br /&gt;&lt;br /&gt;Do no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "11841  Not worth the video rental or the time or the ...          0\n",
       "19602  I would recommend this for anyone who is an ad...          1\n",
       "45519  This film is about two female killers going on...          0\n",
       "25747  The film is a pathetic attempt to remake Ingma...          0\n",
       "42642  What a trip down memory lane.<br /><br />Do no...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "## uncomment these lines if you have dowloaded the original file:\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df[['review', 'sentiment']].to_csv('shuffled_movie_data.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define a simple `tokenizer` that splits the text into individual word tokens. Furthermore, we will use some simple regular expression to remove HTML markup and all non-letter characters but \"emoticons,\" convert the text to lower case, remove stopwords, and apply the Porter stemming algorithm to convert the words into their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# stop = stopwords.words('english')\n",
    "# porter = PorterStemmer()\n",
    "\n",
    "# def tokenizer(text):\n",
    "#     text = re.sub('<[^>]*>', '', text)\n",
    "#     emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "#     text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "#     text = [w for w in text.split() if w not in stop]\n",
    "#     tokenized = [porter.stem(w) for w in text]\n",
    "#     return text\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower())\n",
    "    text = [w for w in text.split()]\n",
    "    tokenized = [wordnet_lemmatizer.lemmatize(w) for w in text]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it at try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'test']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('This :) is a <a> test! :-)</br>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning (SciKit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a generator that returns the document body and the corresponding class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding=\"utf8\") as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conform that the `stream_docs` function fetches the documents as intended, let us execute the following code snippet before we implement the `get_minibatch` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"Not worth the video rental or the time or the occasional efforts.<br /><br />*Makeup that a child can do. *Acting was over done...poor directing. *Editing was very choppy...many things made no sense or just seemed gratuitous. *Sound was badly dubbed. *Music was highly inappropriate. *Casting was extremely off...must have been on crack. *Zombies that talk let alone...drive, dance, work...just pisses me off. *And the bad guy...Holy Crap! As horribly casted as he was...he was the best looking zombie of all. Which doesn\\'t say much.<br /><br />The Cover Art was good but very deceiving...as was the Main Menu of the DVD...great artwork and music.<br /><br />DON\"\"T BOTHER!\"',\n",
       " 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='shuffled_movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we confirmed that our `stream_docs` functions works, we will now implement a `get_minibatch` function to fetch a specified number (`size`) of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text, label = next(doc_stream)\n",
    "        docs.append(text)\n",
    "        y.append(label)\n",
    "    return np.array(docs), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from gensim) (1.15.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: requests in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.19.1)\n",
      "Requirement already satisfied: bz2file in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
      "Requirement already satisfied: boto>=2.32 in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: boto3 in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (1.9.45)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2.7)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.45 in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.45)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.45->boto3->smart-open>=1.2.1->gensim) (2.7.3)\n",
      "Requirement already satisfied: docutils>=0.10 in /home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.45->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
      "\u001b[31mtwisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Done     \n",
      "\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "doc_generator = stream_docs(path='shuffled_movie_data.csv')\n",
    "\n",
    "sentences = []\n",
    "labels    = []\n",
    "lengths   = []\n",
    "for idx, review in enumerate(doc_generator):\n",
    "    toVec = tokenizer(review[0])\n",
    "    sentences.append(toVec)\n",
    "    labels.append(review[1])\n",
    "    lengths.append(len(toVec))\n",
    "    sys.stdout.write('\\r{:5.2f}%'.format(100*(idx+1)/50000))\n",
    "sys.stdout.write('\\rDone     \\n\\n')  \n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained!\n"
     ]
    }
   ],
   "source": [
    "emb_size = 40\n",
    "\n",
    "model = Word2Vec(sentences, size=emb_size, window=5, min_count=5, workers=4)\n",
    "print('trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'woman' is similar to 'girl' with a score of 0.8838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpalominop/anaconda3/envs/SI/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "sim = model.wv.most_similar(positive=['woman'], topn=1)\n",
    "print(\"'woman' is similar to '{}' with a score of {:1.4f}\".format(sim[0][0],sim[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size : 35327\n"
     ]
    }
   ],
   "source": [
    "print('vocabulary size :', len(model.wv.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size :  (40,)\n"
     ]
    }
   ],
   "source": [
    "print('embedding size : ', model.wv['woman'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer:\n",
    "    def __init__(self, model, maxlen):\n",
    "        self.model  = model\n",
    "        self.maxlen = maxlen\n",
    "    def transform(self, reviews_tokenized):\n",
    "        n = len(reviews_tokenized)\n",
    "        vector = np.zeros((n, self.maxlen, emb_size), dtype=np.float16)\n",
    "        for idx, review in enumerate(reviews_tokenized):\n",
    "            for iw, word in enumerate(review):\n",
    "                if word in self.model.wv.vocab:\n",
    "                    vector[idx][iw] = self.model.wv[word]\n",
    "            sys.stdout.write('\\r{:5.2f}%'.format(100*(idx+1)/n))\n",
    "        sys.stdout.write('\\rDone     \\n\\n')                    \n",
    "        vector = vector.reshape((n, -1))\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = max(lengths)\n",
    "vectorizer = ReviewVectorizer(model, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = vectorizer.transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (MAXLEN*emb_size == res.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (50000, 99920)\n",
      "y_train shape:  (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.asarray(labels).reshape((-1, 1))\n",
    "X_train = res\n",
    "del res\n",
    "print('X_train shape: ',X_train.shape)\n",
    "print('y_train shape: ',y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will make use of the \"hashing trick\" through scikit-learns [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) to create a bag-of-words model of our documents. Details of the bag-of-words model for document classification can be found at  [Naive Bayes and Text Classification I - Introduction and Theory](http://arxiv.org/abs/1410.5329)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# vect = HashingVectorizer(decode_error='ignore', \n",
    "#                          n_features=2**21,\n",
    "#                          preprocessor=None, \n",
    "#                          tokenizer=tokenizer)\n",
    "\n",
    "# Exercise 1: define features based on word embeddings (pre-trained word2vec / Glove/Fastext emebddings can be used)\n",
    "# Define suitable d dimension, and sequence length\n",
    "# https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/\n",
    "\n",
    "\n",
    "# doc_stream = stream_docs(path='shuffled_movie_data.csv')\n",
    "# X_train, y_train = get_minibatch(doc_stream, size=50000)\n",
    "# X_test, y_test = get_minibatch(doc_stream, size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 500) \n",
    "# train_data_features = vectorizer.fit_transform(X_train)\n",
    "# X_train_vec = vectorizer.transform(X_train).toarray()\n",
    "\n",
    "# X_train_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [SGDClassifier]() from scikit-learn, we will can instanciate a logistic regression classifier that learns from the documents incrementally using stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import SGDClassifier\n",
    "# clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "# doc_stream = stream_docs(path='shuffled_movie_data.csv')\n",
    "# Exercise 2: Define at least a Three layer neural network. Define its structure (number of hidden neurons, etc)\n",
    "# Define a nonlinear function for hidden layers.\n",
    "# Define a suitable loss function for binary classification\n",
    "# Implement the backpropagation algorithm for this structure\n",
    "# Do not use Keras / Tensorflow /PyTorch etc. libraries\n",
    "# Train the model using SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red Neuronal de 3 capas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train.shape[1]\n",
    "K = 2\n",
    "\n",
    "# Numero de neuronas por capas\n",
    "nn_layer_0 = D\n",
    "nn_layer_1 = 200\n",
    "nn_layer_2 = 200\n",
    "nn_layer_3 = K\n",
    "\n",
    "def initialize_parameters(nn_input_dim, nn_hdim, nn_output_dim):\n",
    "    # First layer weights\n",
    "    np.random.seed(0)\n",
    "    mu, sigma = 0, 0.1\n",
    "    \n",
    "    W1 = np.random.normal(mu, sigma, (nn_hdim, nn_input_dim))\n",
    "#     W1 = np.zeros((nn_input_dim, nn_hdim_1))\n",
    "    \n",
    "    # First layer bias\n",
    "#     b1 = np.random.normal(mu, sigma, (nn_hdim_1, 1))\n",
    "    b1 = np.zeros((nn_hdim, 1))\n",
    "    \n",
    "    # Second layer weights\n",
    "    W2 = np.random.normal(mu, sigma, (nn_hdim, nn_hdim))\n",
    "#     W2 = np.zeros((nn_hdim_1, nn_hdim_2))\n",
    "    \n",
    "    # Second layer bias\n",
    "#     b2 = np.random.normal(mu, sigma, (nn_hdim_2, 1))\n",
    "    b2 = np.zeros((nn_hdim, 1))\n",
    "    \n",
    "    # Thirs layer weights\n",
    "    W3 = np.random.normal(mu, sigma, (nn_output_dim, nn_hdim))\n",
    "#     W3 = np.zeros((nn_hdim_2, nn_output_dim))\n",
    "    \n",
    "    # Second layer bias\n",
    "#     b3 = np.random.normal(mu, sigma, (nn_output_dim, 1))\n",
    "    b3 = np.zeros((nn_output_dim, 1))\n",
    "    \n",
    "    \n",
    "    # Package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non Linear function for hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_deriv(x):\n",
    "    x[x<=0] = 0.0\n",
    "    x[x>0] = 1.0\n",
    "#     for i in range(x.shape[0]):\n",
    "#         x[i] = [(el>0 and 1 or 0) for el in x[i,:]]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non Linear function for output layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1/(1.0+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(z):\n",
    "    \"\"\"\n",
    "    Compute the derivative of the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid_deriv(z)\n",
    "    \"\"\"\n",
    "    s = sigmoid(z)*(1-sigmoid(z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function for binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y, y_hat):\n",
    "    m = y.shape[1]\n",
    "    L = 1/m*np.sum(0.5*(y-y_hat)*(y-y_hat))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def delta_cross_entropy(y, y_hat):    \n",
    "#     ### START CODE HERE ### (≈ 4 lines of code)\n",
    "\n",
    "#     delta = (y_hat-y)*sigmoid_deriv()\n",
    "#     ### END CODE HERE ###\n",
    "    \n",
    "#     return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_two_layer(W1, b1, W2, b2, W3, b3, X, y, use_reg=False, reg_lambda=0.01):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dW -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]\n",
    "#     print(\"propagate_two_layer X.shape: \", m)\n",
    "    num_examples = X.shape[0]\n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 5 lines of code), one hidden layer, first layer with RELU, activation: softmax\n",
    "\n",
    "#     X12 = np.maximum(np.dot(W1.T, X.T)+b1, 0)\n",
    "    A0 = X.T\n",
    "#     A1 = np.tanh(np.dot(W1.T, A0)+b1)\n",
    "    Z1 = np.dot(W1, A0)+b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1)+b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2)+b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    cost = compute_cost(y.T, y_hat=A3)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if use_reg:\n",
    "        ### START CODE HERE ###  (≈ 2 add regularization here)\n",
    "        \n",
    "        reg_cost = 0.5 * reg_lambda * np.sum(W1*W1) + 0.5 * reg_lambda * np.sum(W2*W2) + 0.5 * reg_lambda * np.sum(W3*W3)\n",
    "        cost = cost + reg_cost\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    \n",
    "#     dz3 = delta_cross_entropy(y=y, y_hat=A3)\n",
    "    dz3 = (A3-y.T)*sigmoid_deriv(Z3)\n",
    "  \n",
    "    # backpropate the gradient to the parameters\n",
    "    # first backprop into parameters W2 and b2\n",
    "    dW3 = np.dot(dz3, A2.T)/m\n",
    "    db3 = np.sum(dz3, axis=1)[:, np.newaxis]/m\n",
    "    \n",
    "    # next backprop into hidden layer\n",
    "#     dhidden = ...\n",
    "    dz2 = np.dot(W3.T, dz3)*relu_deriv(Z2)\n",
    "    dW2 = np.dot(dz2, A1.T)/m\n",
    "    db2 = np.sum(dz2, axis=1)[:, np.newaxis]/m\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # next backprop into hidden layer\n",
    "#     dhidden = ...\n",
    "    dz1 = np.dot(W2.T, dz2)*relu_deriv(Z1)\n",
    "    dW1 = np.dot(dz1, A0.T)/m\n",
    "    db1 = np.sum(dz1, axis=1)[:, np.newaxis]/m\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if use_reg:\n",
    "        ### START CODE HERE ###  (≈ 2 add backprop regularization here)\n",
    "        dW1 += reg_lambda*W1\n",
    "        dW2 += reg_lambda*W2\n",
    "        dW3 += reg_lambda*W3\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    assert(dW1.shape == W1.shape)\n",
    "    assert(db1.dtype == float)\n",
    "    assert(dW2.shape == W2.shape)\n",
    "    assert(db2.dtype == float)\n",
    "    assert(dW3.shape == W3.shape)\n",
    "    assert(db3.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2,\n",
    "             \"dW3\": dW3,\n",
    "             \"db3\": db3}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_two_layer(W1, b1, W2, b2, W3, b3, X, y, num_iterations, learning_rate, use_reg = False, reg_lambda = 0.01, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    use_reg -- use regularization\n",
    "    reg_lambda -- regularization weight\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    cost_i = 0\n",
    "    for ii in range(num_iterations):\n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        grads, cost = propagate_two_layer(W1, b1, W2, b2, W3, b3, X, y, use_reg=use_reg, reg_lambda=reg_lambda)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dW1 = grads[\"dW1\"]\n",
    "        db1 = grads[\"db1\"]\n",
    "        dW2 = grads[\"dW2\"]\n",
    "        db2 = grads[\"db2\"]\n",
    "        dW3 = grads[\"dW3\"]\n",
    "        db3 = grads[\"db3\"]\n",
    "        \n",
    "        # update rule (≈ 4 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        W1 = W1 - learning_rate*dW1\n",
    "        b1 = b1 - learning_rate*db1\n",
    "        W2 = W2 - learning_rate*dW2\n",
    "        b2 = b2 - learning_rate*db2\n",
    "        W3 = W3 - learning_rate*dW3\n",
    "        b3 = b3 - learning_rate*db3\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if ii % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            cost_i += 1\n",
    "        \n",
    "        # Print the cost every 200 training iterations\n",
    "        if print_cost and ii % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(ii, cost))\n",
    "            \n",
    "#         if ((ii>100) and (np.abs(costs[cost_i-2]-costs[cost_i-1])<0.01)):\n",
    "#           break\n",
    "    \n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2,\n",
    "             \"dW3\": dW3,\n",
    "             \"db3\": db3}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_two_layer(W1, b1, W2, b2, W3, b3, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "\n",
    "    A0 = X.T\n",
    "#     A1 = np.tanh(np.dot(W1.T, X.T)+b1)\n",
    "    A1 = relu(np.dot(W1, A0)+b1)\n",
    "    A2 = relu(np.dot(W2, A1)+b2)\n",
    "    A3 = sigmoid(np.dot(W3, A2)+b3)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for ii in range(A3.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "\n",
    "        if(A3[0, ii] > 0.5):\n",
    "            Y_prediction[0, ii] = 1\n",
    "        else:\n",
    "            Y_prediction[0, ii] = 0\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_two_layer(X_train, y_train, K=2, h_neurons=100, num_iterations=2000, learning_rate=0.5, use_reg=False, reg_lambda=0.01, init_type='random', print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize parameters with zeros (≈ 2 lines of code)\n",
    "    Y_prediction_train = 0\n",
    "    costs_list = []\n",
    "    model = initialize_parameters(D, h_neurons, K)\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3']\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    for i in range(10):\n",
    "        X_train_batch = X_train[(i*5000):((i+1)*5000),:]\n",
    "        y_train_batch = y_train[(i*5000):((i+1)*5000)]\n",
    "\n",
    "#         print(\"X_train_batch.shape: \", X_train_batch.shape)\n",
    "        parameters, grads, costs = optimize_two_layer(W1, b1, W2, b2, W3, b3, X_train_batch, y_train_batch, num_iterations, learning_rate, use_reg = use_reg, reg_lambda = reg_lambda, print_cost = print_cost)\n",
    "        costs_list.append(costs)\n",
    "        # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        W3 = parameters[\"W3\"]\n",
    "        b3 = parameters[\"b3\"]\n",
    "\n",
    "        Y_prediction_train = predict_two_layer(W1, b1, W2, b2, W3, b3, X_train_batch)\n",
    "\n",
    "        # Print train/test Errors\n",
    "        print('training accuracy: %.2f' % (np.mean(Y_prediction_train == y_train_batch.T)))\n",
    "    \n",
    "    d = {\"costs\": costs_list,\n",
    "         \"W1\" : W1, \n",
    "         \"b1\" : b1,\n",
    "         \"W2\" : W2, \n",
    "         \"b2\" : b2,\n",
    "         \"W3\" : W3, \n",
    "         \"b3\" : b3,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations,\n",
    "         \"predictions\": Y_prediction_train}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n",
      "Automatic pdb calling has been turned ON\n",
      "Cost after iteration 0: 10.142763\n",
      "Cost after iteration 100: 8.280209\n",
      "Cost after iteration 200: 6.737145\n",
      "Cost after iteration 300: 5.495683\n",
      "Cost after iteration 400: 4.497930\n",
      "Cost after iteration 500: 3.683114\n",
      "Cost after iteration 600: 3.016605\n",
      "Cost after iteration 700: 2.471119\n",
      "Cost after iteration 800: 2.024502\n",
      "Cost after iteration 900: 1.658542\n",
      "training accuracy: 1.00\n",
      "Cost after iteration 0: 1.512807\n",
      "Cost after iteration 100: 1.119222\n",
      "Cost after iteration 200: 0.915909\n",
      "Cost after iteration 300: 0.750900\n",
      "Cost after iteration 400: 0.615611\n",
      "Cost after iteration 500: 0.504955\n",
      "Cost after iteration 600: 0.414385\n",
      "Cost after iteration 700: 0.340110\n",
      "Cost after iteration 800: 0.279340\n",
      "Cost after iteration 900: 0.229534\n",
      "training accuracy: 0.52\n",
      "Cost after iteration 0: 0.354341\n",
      "Cost after iteration 100: 0.234879\n",
      "Cost after iteration 200: 0.169749\n",
      "Cost after iteration 300: 0.122463\n",
      "Cost after iteration 400: 0.098243\n",
      "Cost after iteration 500: 0.080133\n",
      "Cost after iteration 600: 0.065805\n",
      "Cost after iteration 700: 0.054215\n",
      "Cost after iteration 800: 0.044813\n",
      "Cost after iteration 900: 0.037062\n",
      "training accuracy: 1.00\n",
      "Cost after iteration 0: 0.145273\n",
      "Cost after iteration 100: 0.193014\n",
      "Cost after iteration 200: 0.154838\n",
      "Cost after iteration 300: 0.032820\n",
      "Cost after iteration 400: 0.022231\n",
      "Cost after iteration 500: 0.017952\n",
      "Cost after iteration 600: 0.014943\n",
      "Cost after iteration 700: 0.012513\n",
      "Cost after iteration 800: 0.010644\n",
      "Cost after iteration 900: 0.009130\n",
      "training accuracy: 1.00\n",
      "Cost after iteration 0: 0.116683\n",
      "Cost after iteration 100: 0.077636\n",
      "Cost after iteration 200: 0.063732\n",
      "Cost after iteration 300: 0.017336\n",
      "Cost after iteration 400: 0.012602\n",
      "Cost after iteration 500: 0.010445\n",
      "Cost after iteration 600: 0.008984\n",
      "Cost after iteration 700: 0.007743\n",
      "Cost after iteration 800: 0.006815\n",
      "Cost after iteration 900: 0.006080\n",
      "training accuracy: 1.00\n",
      "Cost after iteration 0: 0.113513\n",
      "Cost after iteration 100: 0.090429\n",
      "Cost after iteration 200: 0.052523\n",
      "Cost after iteration 300: 0.059612\n",
      "Cost after iteration 400: 0.014969\n",
      "Cost after iteration 500: 0.012507\n",
      "Cost after iteration 600: 0.010981\n",
      "Cost after iteration 700: 0.009506\n",
      "Cost after iteration 800: 0.008310\n",
      "Cost after iteration 900: 0.007451\n",
      "training accuracy: 1.00\n",
      "Cost after iteration 0: 0.118492\n",
      "Cost after iteration 100: 0.144041\n",
      "Cost after iteration 200: 0.048730\n",
      "Cost after iteration 300: 0.013547\n",
      "Cost after iteration 400: 0.009846\n",
      "Cost after iteration 500: 0.008130\n",
      "Cost after iteration 600: 0.007058\n",
      "Cost after iteration 700: 0.006295\n",
      "Cost after iteration 800: 0.005628\n",
      "Cost after iteration 900: 0.005158\n",
      "training accuracy: 1.00\n",
      "Cost after iteration 0: 0.112204\n",
      "Cost after iteration 100: 0.067612\n",
      "Cost after iteration 200: 0.030960\n",
      "Cost after iteration 300: 0.047434\n",
      "Cost after iteration 400: 0.013933\n",
      "Cost after iteration 500: 0.011750\n",
      "Cost after iteration 600: 0.009958\n",
      "Cost after iteration 700: 0.008685\n",
      "Cost after iteration 800: 0.007660\n",
      "Cost after iteration 900: 0.006829\n",
      "training accuracy: 0.99\n",
      "Cost after iteration 0: 0.113973\n",
      "Cost after iteration 100: 0.114360\n",
      "Cost after iteration 200: 0.031224\n",
      "Cost after iteration 300: 0.017985\n",
      "Cost after iteration 400: 0.013194\n",
      "Cost after iteration 500: 0.011037\n",
      "Cost after iteration 600: 0.009780\n",
      "Cost after iteration 700: 0.008565\n",
      "Cost after iteration 800: 0.007481\n",
      "Cost after iteration 900: 0.006792\n",
      "training accuracy: 0.49\n",
      "Cost after iteration 0: 0.251941\n",
      "Cost after iteration 100: 0.098085\n",
      "Cost after iteration 200: 0.149033\n",
      "Cost after iteration 300: 0.079577\n",
      "Cost after iteration 400: 0.161572\n",
      "Cost after iteration 500: 0.053826\n",
      "Cost after iteration 600: 0.070231\n",
      "Cost after iteration 700: 0.020407\n",
      "Cost after iteration 800: 0.016713\n",
      "Cost after iteration 900: 0.014311\n",
      "training accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "%xmode Plain\n",
    "%pdb on\n",
    "d = train_two_layer(X_train, y_train, K=1,  h_neurons=20, num_iterations=1000, learning_rate= 1, use_reg=True, reg_lambda=1e-3, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "#import pyprind\n",
    "#pbar = pyprind.ProgBar(45)\n",
    "\n",
    "# classes = np.array([0, 1])\n",
    "# for _ in range(45):\n",
    "#     X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "#     X_train = vect.transform(X_train).toarray()\n",
    "#     clf.partial_fit(X_train, y_train, classes=classes)\n",
    "#     #pbar.update()\n",
    "    \n",
    "Y_prediction_train = predict_two_layer(d[\"W1\"], d[\"b1\"], d[\"W2\"], d[\"b2\"], d[\"W3\"], d[\"b3\"], X_train[0:10000,:])\n",
    "print('training accuracy: %.2f' % (np.mean(Y_prediction_train == y_train[0:10000].T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your machine, it will take about 2-3 minutes to stream the documents and learn the weights for the logistic regression model to classify \"new\" movie reviews. Executing the preceding code, we used the first 45,000 movie reviews to train the classifier, which means that we have 5,000 reviews left for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "# X_test = vect.transform(X_test)\n",
    "# print('Accuracy: %.3f' % clf.score(X_test, y_test))\n",
    "#Exercise 3: compare  with your Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that the predictive performance, an accuracy of ~87%, is quite \"reasonable\" given that we \"only\" used the default parameters and didn't do any hyperparameter optimization. \n",
    "\n",
    "After we estimated the model perfomance, let us use those last 5,000 test samples to update our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we successfully trained a model to predict the sentiment of a movie review. Unfortunately, if we'd close this IPython notebook at this point, we'd have to go through the whole learning process again and again if we'd want to make a prediction on \"new data.\"\n",
    "\n",
    "So, to reuse this model, we could use the [`pickle`](https://docs.python.org/3.5/library/pickle.html) module to \"serialize a Python object structure\". Or even better, we could use the [`joblib`](https://pypi.python.org/pypi/joblib) library, which handles large NumPy arrays more efficiently.\n",
    "\n",
    "To install:\n",
    "conda install -c anaconda joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "if not os.path.exists('./pkl_objects'):\n",
    "    os.mkdir('./pkl_objects')\n",
    "    \n",
    "joblib.dump(vect, './vectorizer.pkl')\n",
    "joblib.dump(clf, './clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code above, we \"pickled\" the `HashingVectorizer` and the `SGDClassifier` so that we can re-use those objects later. However, `pickle` and `joblib` have a known issue with `pickling` objects or functions from a `__main__` block and we'd get an `AttributeError: Can't get attribute [x] on <module '__main__'>` if we'd unpickle it later. Thus, to pickle the `tokenizer` function, we can write it to a file and import it to get the `namespace` \"right\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tokenizer.py\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import tokenizer\n",
    "joblib.dump(tokenizer, './tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us restart this IPython notebook and check if the we can load our serialized objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "tokenizer = joblib.load('./tokenizer.pkl')\n",
    "vect = joblib.load('./vectorizer.pkl')\n",
    "clf = joblib.load('./clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the `tokenizer`, `HashingVectorizer`, and the tranined logistic regression model, we can use it to make predictions on new data, which can be useful, for example, if we'd want to embed our classifier into a web application -- a topic for another IPython notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ['I did not like this movie']\n",
    "X = vect.transform(example)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ['I loved this movie']\n",
    "X = vect.transform(example)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
