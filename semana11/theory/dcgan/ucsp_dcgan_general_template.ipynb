{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEKJvRpmP5vq"
   },
   "source": [
    "# What are GANs?\n",
    "GANs, or [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661), are a framework for estimating generative models. Two models are trained simultaneously by an adversarial process: a Generator, which is responsible for generating data (say, images), and a Discriminator, which is responsible for estimating the probability that an image was drawn from the training data (the image is real), or was produced by the Generator (the image is fake). During training, the Generator becomes progressively better at generating images, until the Discriminator is no longer able to distinguish real images from fake. \n",
    "​\n",
    "![alt text](https://github.com/margaretmz/tensorflow/blob/margaret-dcgan/tensorflow/contrib/eager/python/examples/generative_examples/gans_diagram.png?raw=1)\n",
    "​\n",
    "We will demonstrate this process end-to-end on MNIST. Below is an animation that shows a series of images produced by the Generator as it was trained for 50 epochs. Overtime, the generated images become increasingly difficult to distinguish from the training set.\n",
    "​\n",
    "To learn more about GANs, we recommend MIT's [Intro to Deep Learning](http://introtodeeplearning.com/) course, which includes a lecture on Deep Generative Models ([video](https://youtu.be/JVb54xhEw6Y) | [slides](http://introtodeeplearning.com/materials/2018_6S191_Lecture4.pdf)). Now, let's head to the code!\n",
    "​\n",
    "![sample output](https://tensorflow.org/images/gan/dcgan.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJ78VKZbQAGj"
   },
   "source": [
    "## The Generator Model\n",
    "\n",
    "The generator is responsible for creating convincing images that are good enough to fool the discriminator. The network architecture for the generator consists of [Conv2DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose) (Upsampling) layers. We start with a fully connected layer and upsample the image two times in order to reach the desired image size of 28x28x1. We increase the width and height, and reduce the depth as we move through the layers in the network. We use [Leaky ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LeakyReLU) activation for each layer except for the last one where we use a tanh activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m17lvqc5QDVN"
   },
   "source": [
    "## The Discriminator model\n",
    "\n",
    "The discriminator is responsible for distinguishing fake images from real images. It's similar to a regular CNN-based image classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bC14zxiirgBF"
   },
   "source": [
    "# How to train a GAN?\n",
    "\n",
    "Follow the tips/tricks given in https://github.com/soumith/ganhacks and in [medium](https://medium.com/@utk.is.here/keep-calm-and-train-a-gan-pitfalls-and-tips-on-training-generative-adversarial-networks-edd529764aa9) to find out the best design and strategies to train your GAN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cr6s-QP5QGXB"
   },
   "source": [
    "# Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hENjv7_ZQehi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import imageio\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy.ndimage\n",
    "import cv2\n",
    "\n",
    "from scipy import misc\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras.engine import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from skimage.transform import resize\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0IFIy4f4QlTy"
   },
   "source": [
    "# Mounting Google Drive (gdrive)\n",
    "\n",
    "This ipnb save the training process on you google drive account. You drive must contain the following directory structure in \"My Drive\": /DCGAN/MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "F3zhrHLGQl82",
    "outputId": "da4a4cc0-f073-46b7-aabb-e5ab6acb7770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /gdrive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1akKrC2Vp97"
   },
   "source": [
    "# Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JZbVEmS0Ttcr"
   },
   "outputs": [],
   "source": [
    "def imread(f):\n",
    "    x = misc.imread(f)\n",
    "    x = x[center_height:center_height + width, :]\n",
    "    x = misc.imresize(x, (img_dim, img_dim))\n",
    "\n",
    "    if len(x.shape) == 2:\n",
    "        x = np.expand_dims(x, axis=2)\n",
    "\n",
    "    return x.astype(np.float32) / 255 * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yHpqJls_UYRP"
   },
   "outputs": [],
   "source": [
    "#imgs = glob.glob('/path/to/images/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e69Uv89-T0mR"
   },
   "outputs": [],
   "source": [
    "def data_generator(batch_size=64):\n",
    "    X = []\n",
    "    while True:\n",
    "        np.random.shuffle(imgs)\n",
    "        for f in imgs:\n",
    "            X.append(imread(f))\n",
    "            if len(X) == batch_size:\n",
    "                X = np.array(X)\n",
    "                yield X\n",
    "                X = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1KCvWQsuT1f7"
   },
   "outputs": [],
   "source": [
    "def mnist_generator(X_data, batch_size=64):\n",
    "  idx = np.random.randint(0, X_data.shape[0], batch_size)\n",
    "  imgs = X_data[idx]\n",
    "  return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaRNbfq4Qn_D"
   },
   "source": [
    " # DCGAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcxMxQ7SQxAy"
   },
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "\n",
    "    def __init__(self, imdim=128, imchannels=1, im_minsze=4, latent_dim=128, filter_sze=5, batch_size=32,\n",
    "                 disc_filters=64, disc_dropout=0.25, disc_bn_momentum=0.8, disc_add_noise=False,\n",
    "                 gen_bn_momentum=0.8, gen_act = 'relu',\n",
    "                 iter_disc=1, iter_gen=1, save_interval=100, model_name='dc_gan', dataset_name='mnist', dataset_path='', dataset_out=''):\n",
    "        # Input shape\n",
    "        self.imdim = imdim\n",
    "        self.imrows = self.imdim\n",
    "        self.imcols = self.imdim\n",
    "        self.imchannels = imchannels\n",
    "        self.img_min_sze = im_minsze\n",
    "        self.img_shape = (self.imrows, self.imcols, self.imchannels)\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.filter_sze = filter_sze\n",
    "        self.disc_filters = disc_filters\n",
    "        self.disc_depth = 3\n",
    "        self.disc_dropout = disc_dropout\n",
    "        self.disc_bn_momentum = disc_bn_momentum\n",
    "        self.disc_add_noise = disc_add_noise\n",
    "        self.gen_act = gen_act\n",
    "        self.gen_filters = 64\n",
    "        self.gen_bn_momentum = gen_bn_momentum\n",
    "        self.optimizer = Adam(2e-4, 0.5)\n",
    "        self.iter_disc = iter_disc  # number of discriminator network updates per adversarial training step\n",
    "        self.iter_gen = iter_gen  # number of generative network updates per adversarial training step\n",
    "        self.save_interval = save_interval\n",
    "\n",
    "        outDir = 'pix{}_ksze{}_zdim{}'.format(self.imdim, self.filter_sze, self.latent_dim)\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = dataset_name\n",
    "        self.model_name = model_name\n",
    "        self.dataset_path = dataset_path\n",
    "        self.logs_dir = os.path.join(self.dataset_path + '/logs/')\n",
    "        self.preds_dir = os.path.join(self.dataset_path +'/preds/')\n",
    "        self.dataset_out = dataset_out #os.path.join(dataset_out + get_timestamp())\n",
    "        self.model_out = os.path.join(self.dataset_out + '/model_{}/'.format(outDir))\n",
    "        self.samples_out = os.path.join(self.dataset_out +'/samples_{}/'.format(outDir))\n",
    "\n",
    "        if not os.path.exists(self.dataset_out):\n",
    "            os.makedirs(self.dataset_out)\n",
    "\n",
    "        if not os.path.exists(self.samples_out):\n",
    "            os.makedirs(self.samples_out)\n",
    "\n",
    "        if not os.path.exists(self.model_out):\n",
    "            os.makedirs(self.model_out)\n",
    "\n",
    "        # Build the discriminator\n",
    "        self.discriminator = ...\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = ...\n",
    "\n",
    "        # Combined model\n",
    "        x_in = ... # define input here\n",
    "        z_in = ... # define input here\n",
    "\n",
    "        # Configuring Discriminator\n",
    "        self.generator.trainable = False\n",
    "\n",
    "        x_real = x_in\n",
    "        x_fake = ... # generate fake image\n",
    "\n",
    "        x_real_score = ... # get discriminator score on real image\n",
    "        x_fake_score = ... # get discriminator score on fake image\n",
    "\n",
    "        self.discriminator_combined = Model(...) # create combined model here.\n",
    "\n",
    "        d_loss = ... # define discriminator loss here\n",
    "        self.discriminator_combined.add_loss(d_loss)\n",
    "        self.discriminator_combined.compile(optimizer=self.optimizer)\n",
    "\n",
    "        # Configuring Generator\n",
    "        self.generator.trainable = True\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        x_real = x_in\n",
    "        x_fake = ... # generate fake image here\n",
    "\n",
    "        x_real_score = ... # get discriminator score on real image\n",
    "        x_fake_score = ... # get discriminator score on fake image\n",
    "\n",
    "        self.generator_combined = Model(...) # create generator model here\n",
    "\n",
    "        g_loss = ... # define generator loss here\n",
    "        self.generator_combined.add_loss(g_loss)\n",
    "        self.generator_combined.compile(optimizer=self.optimizer)\n",
    "\n",
    "        # Displaying final models\n",
    "        self.discriminator_combined.summary()\n",
    "        self.generator_combined.summary()\n",
    "\n",
    "\n",
    "    def build_discriminator(self):\n",
    "      # create discriminator here.\n",
    "      # use sigmoid activation function for output\n",
    "\n",
    "        return d_model\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "      # create generator here.\n",
    "      # use tanh activation function for output\n",
    "\n",
    "        return g_model\n",
    "\n",
    "    def train(self, epochs):\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        \n",
    "        X_train_sze = []\n",
    "        for img in X_train:\n",
    "            X_train_sze.append(np.expand_dims(cv2.resize(img, (32, 32), interpolation=cv2.INTER_CUBIC),axis=2))\n",
    "        X_train_sze = np.array(X_train_sze)\n",
    "        print(X_train.shape)\n",
    "        print(X_train_sze.shape)\n",
    "        X_train = X_train_sze\n",
    "        \n",
    "        #img_generator = data_generator(batch_size)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for ii in range(self.iter_disc):\n",
    "                # Sample noise and generate a batch of new images\n",
    "                z_sample = ...\n",
    "                d_loss = ... # train discriminator here and get loss\n",
    "\n",
    "            for ii in range(self.iter_gen):\n",
    "                z_sample = ...\n",
    "                g_loss = ... # train generator here and get loss\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print('epoch: {}, d_loss: {}, g_loss: {}'.format(epoch, d_loss, g_loss))\n",
    "\n",
    "            if epoch % self.save_interval == 0:\n",
    "                out_file = '{}/epoch_{}.png'.format(self.samples_out, epoch)\n",
    "                self.save_images_noborder(out_file)\n",
    "                #self.save_images_wborder(out_file)\n",
    "                #self.save_model(self.model_out)\n",
    "\n",
    "\n",
    "    def save_images_noborder(self, path):\n",
    "        n = 5\n",
    "        imdim = self.imdim\n",
    "\n",
    "        figure = np.zeros((imdim * n, imdim * n, 3))\n",
    "        for ii in range(n):\n",
    "            for jj in range(n):\n",
    "                z_sample = np.random.randn(1, self.latent_dim)\n",
    "                x_sample = self.generator.predict(z_sample)\n",
    "                digit = x_sample[0]\n",
    "                figure[ii * imdim:(ii + 1) * imdim,\n",
    "                jj * imdim:(jj + 1) * imdim] = digit\n",
    "        figure = (figure + 1) / 2 * 255\n",
    "        figure = np.round(figure, 0).astype(int)\n",
    "        imageio.imwrite(path, figure)\n",
    "        \n",
    "        \n",
    "    def save_images_wborder(self, path):\n",
    "        r, c = 5, 5\n",
    "        z_sample = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(z_sample)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(path)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def save_model(self, path):\n",
    "\n",
    "        def save(model, model_name, path=self.model_out):\n",
    "            model_path = \"{}/{}.json\".format(path, model_name)\n",
    "            weights_path = \"{}/{}_weights.hdf5\".format(path, model_name)\n",
    "            options = {\"file_arch\": model_path,\n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        save(self.generator_combined, \"dcgan_generator_combined\", path)\n",
    "        save(self.discriminator_combined, \"dcgan_discriminator_combined\", path)\n",
    "        save(self.generator, \"dcgan_generator\", path)\n",
    "        save(self.discriminator, \"dcgan_discriminator\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3G9IE4PxRn27"
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1941
    },
    "colab_type": "code",
    "id": "NzbggRCNRoew",
    "outputId": "6fbcd981-11f6-4ebb-e0c0-6c0801d45c39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_81 (InputLayer)        (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 16, 16, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_69 (Conv2D)           (None, 8, 8, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_97 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 215,297\n",
      "Trainable params: 215,041\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_82 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1024)              103424    \n",
      "_________________________________________________________________\n",
      "batch_normalization_98 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "reshape_21 (Reshape)         (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_50 (Conv2DT (None, 8, 8, 128)         204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_99 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_51 (Conv2DT (None, 16, 16, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_100 (Bat (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_52 (Conv2DT (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_101 (Bat (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 32, 32, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 32, 32, 1)         0         \n",
      "=================================================================\n",
      "Total params: 622,401\n",
      "Trainable params: 619,841\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_84 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_83 (InputLayer)           (None, 32, 32, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_80 (Model)                (None, 32, 32, 1)    622401      input_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_79 (Model)                (None, 1)            215297      input_83[0][0]                   \n",
      "                                                                 model_80[1][0]                   \n",
      "==================================================================================================\n",
      "Total params: 432,898\n",
      "Trainable params: 215,041\n",
      "Non-trainable params: 217,857\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_84 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_80 (Model)             (None, 32, 32, 1)         622401    \n",
      "_________________________________________________________________\n",
      "model_79 (Model)             (None, 1)                 215297    \n",
      "=================================================================\n",
      "Total params: 837,698\n",
      "Trainable params: 619,841\n",
      "Non-trainable params: 217,857\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000, 32, 32, 1)\n",
      "epoch: 0, d_loss: 2.207254409790039, g_loss: 0.9866965413093567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Lossy conversion from int64 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, d_loss: 1.2970011234283447, g_loss: 2.5163936614990234\n",
      "epoch: 20, d_loss: 0.35413384437561035, g_loss: 3.9613208770751953\n",
      "epoch: 30, d_loss: 1.149430274963379, g_loss: 4.482166290283203\n",
      "epoch: 40, d_loss: 0.6466071009635925, g_loss: 2.4570999145507812\n",
      "epoch: 50, d_loss: 1.2778167724609375, g_loss: 3.8087234497070312\n",
      "epoch: 60, d_loss: 1.241053581237793, g_loss: 1.7104198932647705\n",
      "epoch: 70, d_loss: 1.51935613155365, g_loss: 3.221017599105835\n",
      "epoch: 80, d_loss: 0.657823920249939, g_loss: 3.0386414527893066\n",
      "epoch: 90, d_loss: 0.530082643032074, g_loss: 2.957447052001953\n",
      "epoch: 100, d_loss: 2.017062187194824, g_loss: 1.529043436050415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Lossy conversion from int64 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "#dcgan = DCGAN(imdim=28, im_minsze=7, latent_dim=100, filter_sze=3, dataset_out='/gdrive/My Drive/data/ucsp-dcgan/mnist/')\n",
    "dcgan = DCGAN(imdim=32, im_minsze=4, latent_dim=100, filter_sze=5, dataset_out='/gdrive/My Drive/data/ucsp-dcgan/mnist/')\n",
    "dcgan.train(epochs=10001)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ucsp_dcgan_general_template.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
